# Real-Time-ETL-Pipeline-Azure-SQL-to-ADLS-Databricks-and-Pyspark-Project-08
In this project we are going to do end to end pyspark project. How to create ETL Pipeline to load data from Azure SQL to Azure Data Lake Storage. 
<div align="center">
  <a href="#">
    <img src="https://github.com/zBalachandar/Real-Time-ETL-Pipeline-Azure-SQL-to-ADLS-Databricks-and-Pyspark-Project-08/blob/016ee9b779f5c1f70d22468f53aefd718b3c1293/Assets/Azure%20portal%20overview.png" alt="Banner" width="720">
  </a>

  <div id="user-content-toc">
    <ul>
      <summary><h1 style="display: inline-block;"> </h1>Databricks and Pyspark Project | Real Time ETL Pipeline Azure SQL to ADLS</summary>
    </ul>
  </div>
  
  <p>Data Processing using DataBricks</p>
</div>
<br>

## üìù Table of Contents
1. [Project Overview](#introduction)
2. [Data Transformation](#data-transformation)
   2.1 [DataBricks Notebook]
3. [KPI's](#data-reporting)
4. [Credits](#credits)
5. [Contact](#contact)

<a name="introduction"></a>
## üî¨ Project Overview

we have Business requirements and we want to do data cleaning and processing using PySpark in Databricks Environment.
we have discussed how we work in real-time in Databricks and PySpark 
we have developed the presentation Dashboard according to the Databricks notebook KPI's. 

### Project live link: https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/4339393889004771/4320089602186732/1662184132138436/latest.html


### üíæ Dataset
Dataset link: https://drive.google.com/file/d/1kbd1Ew8W8m_Dfeq5zKq3nP6kdeKzK61l/view

### Business Requirement.
![image](https://github.com/zBalachandar/Real-Time-ETL-Pipeline-Azure-SQL-to-ADLS-Databricks-and-Pyspark-Project-08/blob/443e4ba8238c72b1b035c4c3df193e8b43e0b3ad/Assets/BR%204.jpg)

### Project steps to follow: 
what we have covered in this project:

- We need to connect the Azure SQL database using Databricks
- Perform some cleaning operations
- Do some insight using data bricks -> optional
- Mount ADLS/Blob location and store the data.
- Analysis of the data according to the business requirement

we have Business requirements and we want to do data cleaning and processing using PySpark in Databricks Environment.
we have discussed how we work in real-time in Databricks and PySpark 


<a name="data-transformation"></a>
### ‚öôÔ∏è Data Transformation
 Data cleaning and processing using PySpark in Databricks Environment.
# DataBricks Notebook.
[DataBricks Notebook- Project Live-link](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/4339393889004771/4320089602186732/1662184132138436/latest.html)

## Google Play Store Pyspark Project Data frame imported
![image]()

## Dataframe
![Dataframe]()


## Data CLEANING PROGRESS STAGE 01
![Data CLEANING PROGRESS]()

## Data CLEANING PROGRESS STAGE 02
![Data CLEANING PROGRESS 02]()

## Pyspark Cleaning coding withregexp
![Pyspark Cleaning coding withregexp]()

## Pyspark Cleaning coding SQL results
![Pyspark Cleaning coding SQL results]()

## Sql results
![sql results]()

<a name="data-reporting"></a>
#üìä Data Analysis
I created an insightful data Analytics.

## 1.SQL query for Top 10apps
![sql query Top 10apps]()

## 2. SQL query for top-installed apps
![SQL query Top installed apps]()

## 3. SQL query for Top Apps
![sql query 3 ]()

## 4. SQL Category-wise query
![sql categorywise query]()

## 5. Top paid apps SQL query
![Top paid apps sql query]()


### üõ†Ô∏è Technologies Used

- **Data processing**: DataBricks -Pyspark

<a name="credits"></a>
## üìã Credits

- This Project is inspired by the video of the [YouTube Channel "Learn by doing it"](https://www.youtube.com/watch?v=pMqnvXgPKlI&list=PLOlK8ytA0MghGmAAT8W2u7VYmICdzeU5t&index=1&t=96s)  

<a name="contact"></a>
## üì® Contact Me

[LinkedIn](https://www.linkedin.com/in/balachandars2022/) ‚Ä¢
[Gmail](balachandar2014elu@gmail.com)  ‚Ä¢

